

#########################################################################################################
## Introduction to Generative AI 
#########################################################################################################

https://www.cloudskillsboost.google/course_sessions/11483366/video/434966


    +--------------------------+
    |           AI             |
    |   +------------------+   |
    |   |       ML         |   | 
    |   |   +----------+   |   |
    |   |   |   Deep   |   |   |
    |   |   | Learning |   |   |
    |   |   +----------+   |   |
    |   |                  |   | 
    |   +------------------+   |
    |                          |
    +--------------------------+



ML Model trained from Data
    - without explicit programming
    - supervised
            - input data 
            - model  <------------------------+
            - predict output                  |
            - compare to training dataset     |
            - update model (reduce error)  ---+
    - unsupervised
            - input data 
            - model  
            - generated example  



Deep Learning uses Artifical Neural Networks
    - interconnected nodes / neurons



    input         hidden      output
    layer         layer        layer

      x
      x            +
      x            +             o
      x            +             o
      x            +             o
      x            +             o
      x            +
      x


- Generative AI subset of deep learning
- LLM subset of deep learning

Deep Learning Model Type:
    - Generative
        - generates new data similar to trained data
        - understands distribution of data
        - predict next word in sequence

    - Discriminative
        - classify or predict
        - trained on labled data
        - learns relationship between features of data points and labels



    Not Gen AI, output is 
        - number
        - discrete
        - class
        - probability

    Is Gen AI, output is 
        - Natural Language
        - Image
        - Audio

    ex. PaLM : Pathways Language Model 
        LaMDA: Language Model for Dialogue Applications


Generative AI: 
    - type of AI that create new content based on what it has learned from existing content
    - process of learning from existing content is training 
        - results in creation of statistical model
    - given a prompt, GenAI uses statistical model to predict what response might be

Generative Language Models learn about patterns in language thru training data 
    - pattern mathing system 


Gen Ai derived from Transformers:
    - Transformer model consists of 
        - encoder: encodes input sequence / passes to decoder
        - decoder: learns to decode the representation for a task 

Hallucinations: words / phrases generated by model that are non-sensical / incorrect

prompt: piece of text givent to LLM as input
            - can be used to control output of model


model types:
    text-to-text
    text-to-image
    text-to-video
    text-to-3D
    text-to-task

ex. Vertex AI: model garden that includes foundation models
    Language: PaLM API for Chat
              PaLM API for Text
              BERT
    Vision:   Stable Diffustion v1-5
              BLIP image captioning 
              CLIP 
              OWL, etc.


Model Garden Task Specific Models 

    Language: Extraction: 
                    - Syntax Analysis
              Classification: 
                    - Entity Analysis
                    - Content Classification
                    - Sentiment Analysis
                    - Entity Sentiment Analysis

    Vision:   Classification: 
                    - Object detector
              Detection:
                    - Occupancy analytics
                    - Person / Vehicle detector
                    - PPE detector
                    - Person blur


Colab: Google's free Jupyter

GenAI Studio: create and deploy GenAI Models 
            - Language / Vision
            - Library of pre-trained models
            - tool to fine tunemodels
            - AI App Builder
                    - create gen AI apps wihout writing code


- Vertext AI Search and Conversation

- MakerSuite: prototype apps using PaLM API
    - model training tool
    - model deployment tool
    - model monitoring tool


## Resources: 



All Readings: Introduction to Generative AI (G-GENAI-I)

[[ Here are the assembled readings on generative AI: ]]

● Ask a Techspert: What is generative AI?
https://blog.google/inside-google/googlers/ask-a-techspert/what-is-generative-ai/

● Build new generative AI powered search & conversational experiences with Gen App
Builder:
https://cloud.google.com/blog/products/ai-machine-learning/create-generative-apps-in-minutes-with-gen-app-builder

● What is generative AI?
https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai

● Google Research, 2022 & beyond: Generative models:
https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html#GenerativeModels


● Building the most open and innovative AI ecosystem:
https://cloud.google.com/blog/products/ai-machine-learning/building-an-open-generative-ai-partner-ecosystem


● Generative AI is here. Who Should Control It?
https://www.nytimes.com/2022/10/21/podcasts/hard-fork-generative-artificial-intelligence.html


● Stanford U & Google’s Generative Agents Produce Believable Proxies of Human
Behaviors:
https://syncedreview.com/2023/04/12/stanford-u-googles-generative-agents-produce-believable-proxies-of-human-behaviours/

● Generative AI: Perspectives from Stanford HAI:
https://hai.stanford.edu/sites/default/files/2023-03/Generative_AI_HAI_Perspectives

● Generative AI at Work:
https://www.nber.org/system/files/working_papers/w31161/w31161.pdf

● The future of generative AI is niche, not generalized:
https://www.technologyreview.com/2023/04/27/1072102/the-future-of-generative-ai-is-niche-not-generalized/


● The implications of Generative AI for businesses:
https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html

● Proactive Risk Management in Generative AI:
https://www2.deloitte.com/us/en/pages/consulting/articles/responsible-use-of-generative-ai.html

● How Generative AI Is Changing Creative Work:
https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work


[[ Here are the assembled readings on large language models: ]]

● NLP's ImageNet moment has arrived: https://thegradient.pub/nlp-imagenet/

● LaMDA: our breakthrough conversation technology:
https://blog.google/technology/ai/lamda/

● Language Models are Few-Shot Learners:
https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf


● PaLM-E: An embodied multimodal language model:
https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html

● PaLM API & MakerSuite: an approachable way to start prototyping and building
generative AI applications:
https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html

● The Power of Scale for Parameter-Efficient Prompt Tuning:
https://arxiv.org/pdf/2104.08691.pdf

● Google Research, 2022 & beyond: Language models:
https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html/LanguageModels


● Solving a machine-learning mystery:
https://news.mit.edu/2023/large-language-models-in-context-learning-0207

[[ Additional Resources: ]]

● Attention is All You Need: https://research.google/pubs/pub46201/

● Transformer: A Novel Neural Network Architecture for Language Understanding:
https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

● Transformer on Wikipedia:
https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#:~:text=Transformers%20were%20introduced%20in%202017,allowing%20training%20on%20larger%20datasets.

● What is Temperature in NLP? https://lukesalamone.github.io/posts/what-is-temperature/

● Model Garden: https://cloud.google.com/model-garden

● Auto-generated Summaries in Google Docs:
https://ai.googleblog.com/2022/03/auto-generated-summaries-in-google-docs.html




#########################################################################################################
## ChatGPT for Everyone	
#########################################################################################################

ChatGPT for Everyone	
https://learnprompting.thinkific.com/courses/take/ChatGPT-for-Everyone/lessons/52250248-introduction


ChatGPT: Conversational Generative AI
    - Generative Pre-Trained Transformer

prompt:  question of set of instructions

# Basics of Prompt Engineering
    - Instruction: main objective or question
    - Context: text / data provided
    - Role: having ChatGPT assume a role to execute for the answer (act as a personal assitent, rust developer, etc.)
    - Format Instructions: Step-by-Step DIY Guide, put summary data in a markdown table 
    - Few-Shot Prompting: Show examples of what to do i.e. how paragraphs were summarized in the past 

# Improving Prompts
    - "Say it in the style of XYZ"
    Prompt Engineering:  Refine prompt over time >> Conversational Prompt Engineering


# Advanced ChatGPT Interface

    - bottom left > account name
    - click > menu: Custom Instructions 
            - allows for additional context
            - shape how ChatGPT responds:
                - What would you like ChatGPT to know about you to proivde better responses (location, work, hobbies, goals, etc)
                - How would you like ChatGPT to respond (how formal or casual, how long / short, how addressed, etc.)

    - history section on left
        - each chat saved
        - NOTE: Chats will be used to train ChatGPT
                - To Turn Off  Account > Settings > Data Controls > Chat History & Training [Off]

    - Share Chat: Top Right > Copy Link 

# ChatGPT's Advanced Features on ChatGPT Plus Accounts
    - Using ChatGPT Vision Capabilities
            - upload image and ask questions about it (ChatGPT 4)
    - Access the Internet
    - Advanced Data Analytics
            - upload a pdf, xls and analyze 
    - DALL-E 3

# Limitations
    - Hallucinations: Doesn't now correct answer but answers anyway
        ex. Which country is bigger, Nigeria or Chad ? 

    - Citing sources:  ChatGPT can make up sources 
        - use proper context

# Bias
    - Training data has been trained on biased data 
        - Prompt to "not be biased" and fine tune 

# Data Privacy
    - ChatGPT records prompts
        - turn off as above
    - ChatGPT does not record or improve model with data from Enterprise Plan

Certificate of completion:
https://learnprompting.thinkific.com/account/certificates
https://learnprompting.thinkific.com/certificates/djnxpzyykn





#########################################################################################################
## Generative AI for Everyone	
#########################################################################################################

https://www.coursera.org/learn/generative-ai-for-everyone/home/week/1

[week 1]
- AI is a set of tools 
    - Supervised learning (labeling things)
    - Unsupervised learning 
    - Reinforcement learning 
    - Generative AI

Supervised learning  (labeling things)
    Input   -> Output


    Input (A)           Output (B)             |   Application
    -------------------------------------------|----------------------------------
     Email                 Spam?  (0/1)        |  Spam filtering
     Ad, user info         Click? (0/1)        |  Online advertising
     Image, radar info   Position other cars   |  Self-driving car
     x-ray image           Diagnosis           |  Healthcare
     image of phone        Defect? (0/1)       |  Visual inspection
     audio recording       Text transcript     |  Speech recognition
     restaurant reviews    Sentiment (neg/pos) |  Reputation monitoring

2010 - 2020: Large scale supervised learning
    Small AI models:  larger data didn't not result in better performance
    Large AI models:  larger data DID result in better performance

    i.e. Let's build large models and feed them ALOT of data
            - pathway to Generative AI


How LLMs work: 
    - LLMs are built by using supervised learning (A -> B) to repeatedly predict the next word. 

        ex. My favorite food is a bagel with cream cheese
    Input (A)  [prompt]                       | Output (B)  [response]
    ----------------------------------------- | ----------------------------
    My favorite food is a                     |  bagel
    My favorite food is a bagel               |  with 
    My favorite food is a bagel with          |  cream  
    My favorite food is a bagel with cream    |  cheese 

    When train an AI system on hundreds of billions of words, get an LLM


Uses for LLMs:
    - writing / brainstorming
    - reading / read and summerize text (emails, etc.)
    - chatting / chatbots
    web  vs.  app 

Gen AI Applications:
  Text:
    - brainstorming
    - writing emails / articles
    - translations (Priate English)

  Reading:
    - proofreading
    - summary
    - email analysis 
    - sentiment / reputation monitoring
    NOTE: refine prompts with additional information

  Chatting:
    - customer service


LLMs: Can vs. Can't Do
Can:
    - can a college grad execute the prompt ?  (guideline)
            - add data to complete the task
    - knowledge cutoffs, LLMs training stops at a certain date
    - Hallucinations: ask about well known people and you may get dubious quotes
    - input / output length is limited
    - Does not work well with structured (tabular) data (used supervised data for this)
    - Works best with unstructured data 
    - Bias / Toxicity


Tips for prompting:

    - Be detailed and specific 
            - Give enough context to complete the task
            - describe taks in detail

    - Guide the model to think through its answer
            - give steps thru the answer 
                    Brainstorm 5 names for a new baby toy.
                    Step 1: Come up with 5 fun, joyful words that relate to human babies.
                    Step 2: For each word, come up with a rhyming name for a toy.
                    Step 3: Fore each toy name, add a fun relevant emoji.
            
    - Experiment and iterate
            - develop a process to iterate 
            - improve prompts

            Idea -----> Prompt -----> LLM Response  ----->  Refine --+
             ^-------------------------------------------------------+
                        (add more detail or detailed task)

Image Generation :

   Image Generation (diffusion model) 
        - learns from images on the Internet (via supervised learning)
        - take an image and then add "noise" to the image
            - teach model to take "noisy" image and generate less-noisy image ---+
                ^------------------------- REPEAT -------------------------------+ 
                NOTE: typically 100 steps for a diffusion model

        - image has text caption associated with it 
        - to generate new image 
            - image is pure noisy 
            - input prompt from user 
            - diffuse to lessy noisy image (repeat per above)


[week 2]
MAP LAST HERE
https://www.coursera.org/learn/generative-ai-for-everyone/lecture/LfGc4/using-generative-ai-in-software-applications

[week 3]







#########################################################################################################
## ChatGPT Prompt Engineering for Developers
#########################################################################################################

https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction

https://github.com/openai/openai-cookbook



Two Types of LLMs:

Base LLM                                            Instruction Tuned LLM 
--------------                                      ---------------------------------------
Predicts enxt word, based on text training data     Tries to follow instruction 
                                                    - fine-tune on instructions
                                                    - Trained with instructions and good attempts to follow instructions
                                                    - RHLF: Reinforcement Learning with Human Feedback

NOTE: Are the instructions clear / detailed / specified enough to return viable responses


## Guidelines

Principles:
    1. Write clear and specific instructions
    2. Give model time to think

NOTE: Check jupyter notebooks in code directory 
    i.e. ~/AI_COURSES/code/L2_guidelines.ipynb



Principle 1. Write clear and specific instructions
    clear != short 

# Tactic 1: use delimeters 
    triple quotes: """
    triple backticks: """
    triple backticks: """

example code: 

text = f"""
You should express what you want a model to do by \ 
providing instructions that are as clear and \ 
specific as you can possibly make them. \ 
This will guide the model towards the desired output, \ 
and reduce the chances of receiving irrelevant \ 
or incorrect responses. Don't confuse writing a \ 
clear prompt with writing a short prompt. \ 
In many cases, longer prompts provide more clarity \ 
and context for the model, which can lead to \ 
more detailed and relevant outputs.
"""
prompt = f"""
Summarize the text delimited by triple backticks \ 
into a single sentence.
```{text}```
"""
response = get_completion(prompt)
print(response)


# Tactic 2: ask for a structured output
    - i.e. HTML, JSON

ex. 
prompt = f"""
Generate a list of three made-up book titles along \ 
with their authors and genres. 
Provide them in JSON format with the following keys: 
book_id, title, author, genre.
"""
response = get_completion(prompt)
print(response)


# Tactic 3: Ask model to check if the conditions were satisfied

ex.
text_1 = f"""
Making a cup of tea is easy! First, you need to get some \ 
water boiling. While that's happening, \ 
grab a cup and put a tea bag in it. Once the water is \ 
hot enough, just pour it over the tea bag. \ 
Let it sit for a bit so the tea can steep. After a \ 
few minutes, take out the tea bag. If you \ 
like, you can add some sugar or milk to taste. \ 
And that's it! You've got yourself a delicious \ 
cup of tea to enjoy.
"""
prompt = f"""
You will be provided with text delimited by triple quotes. 
If it contains a sequence of instructions, \ 
re-write those instructions in the following format:

Step 1 - ...
Step 2 - …
…
Step N - …

If the text does not contain a sequence of instructions, \ 
then simply write \"No steps provided.\"

\"\"\"{text_1}\"\"\"
"""
response = get_completion(prompt)
print("Completion for Text 1:")
print(response)


-- OR -- 

text_1 = f"""
Ahhh forget it,  just drink some beers! 
"""
prompt = f"""
You will be provided with text delimited by triple quotes. 
If it contains a sequence of instructions, \ 
re-write those instructions in the following format:

Step 1 - ...
Step 2 - …
…
Step N - …

If the text does not contain a sequence of instructions, \ 
then simply write \"No steps provided.\"

\"\"\"{text_1}\"\"\"
"""
response = get_completion(prompt)
print("Completion for Text 1:")
print(response)


# Tactic 4: "Few-shot" prompting
    - Give successful examples of completing tasks 
      Then ask model to perform the task

prompt = f"""
Your task is to answer in a consistent style.

<child>: Teach me about patience.

<grandparent>: The river that carves the deepest \ 
valley flows from a modest spring; the \ 
grandest symphony originates from a single note; \ 
the most intricate tapestry begins with a solitary thread.

<child>: Teach me about resilience.
"""
response = get_completion(prompt)
print(response)


Principle 2. Give model time to think

# Tactic 1: Specify the steps required to complete a task

text = f"""
In a charming village, siblings Jack and Jill set out on \ 
a quest to fetch water from a hilltop \ 
well. As they climbed, singing joyfully, misfortune \ 
struck—Jack tripped on a stone and tumbled \ 
down the hill, with Jill following suit. \ 
Though slightly battered, the pair returned home to \ 
comforting embraces. Despite the mishap, \ 
their adventurous spirits remained undimmed, and they \ 
continued exploring with delight.
"""
# example 1
prompt_1 = f"""
Perform the following actions: 
1 - Summarize the following text delimited by triple \
backticks with 1 sentence.
2 - Translate the summary into French.
3 - List each name in the French summary.
4 - Output a json object that contains the following \
keys: french_summary, num_names.

Separate your answers with line breaks.

Text:
```{text}```
"""
response = get_completion(prompt_1)
print("Completion for prompt 1:")
print(response)


# Tactic 2: Instruct the model to work out of its own solution before rushing to conclusion
prompt = f"""
Your task is to determine if the student's solution \
is correct or not.
To solve the problem do the following:
- First, work out your own solution to the problem including the final total. 
- Then compare your solution to the student's solution \ 
and evaluate if the student's solution is correct or not. 
Don't decide if the student's solution is correct until 
you have done the problem yourself.

Use the following format:
Question:
```
question here
```
Student's solution:
```
student's solution here
```
Actual solution:
```
steps to work out the solution and your solution here
```
Is the student's solution the same as actual solution \
just calculated:
```
yes or no
```
Student grade:
```
correct or incorrect
```

Question:
```
I'm building a solar power installation and I need help \
working out the financials. 
- Land costs $100 / square foot
- I can buy solar panels for $250 / square foot
- I negotiated a contract for maintenance that will cost \
me a flat $100k per year, and an additional $10 / square \
foot
What is the total cost for the first year of operations \
as a function of the number of square feet.
``` 
Student's solution:
```
Let x be the size of the installation in square feet.
Costs:
1. Land cost: 100x
2. Solar panel cost: 250x
3. Maintenance cost: 100,000 + 100x
Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
```
Actual solution:
"""
response = get_completion(prompt)
print(response)

To calculate the total cost for the first year of operations, we need to add up the costs of land, solar panels, and maintenance.

1. Land cost: $100 / square foot
The cost of land is $100 multiplied by the size of the installation in square feet.

2. Solar panel cost: $250 / square foot
The cost of solar panels is $250 multiplied by the size of the installation in square feet.

3. Maintenance cost: $100,000 + $10 / square foot
The maintenance cost is a flat fee of $100,000 per year, plus $10 multiplied by the size of the installation in square feet.

Total cost: Land cost + Solar panel cost + Maintenance cost

Let's calculate the total cost using the actual solution:

Total cost = (100 * x) + (250 * x) + (100,000 + (10 * x))
           = 100x + 250x + 100,000 + 10x
           = 360x + 100,000

Is the student's solution the same as the actual solution just calculated:
No

Student grade:
Incorrect




Model Limitations: Hallucinations

NOTE:  Example of a Hallucination.  
    - Boie is a real company but the product IS NOT REAL !!

Tactic to reduct hallucinations:
    - find relevant info and then answer with relevant information


prompt = f"""
Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie
"""
response = get_completion(prompt)
print(response)

The AeroGlide UltraSlim Smart Toothbrush by Boie is a technologically advanced toothbrush designed to provide a superior 
brushing experience. Boie is a company known for its innovative oral care products, and the AeroGlide UltraSlim Smart 
Toothbrush is no exception.

One of the standout features of this toothbrush is its ultra-slim design. The brush head is only 2mm thick, making it 
much thinner than traditional toothbrushes. This slim profile allows for better access to hard-to-reach areas of the mouth,
 ensuring a thorough and effective clean.

The AeroGlide UltraSlim Smart Toothbrush also incorporates smart technology. It connects to a mobile app via Bluetooth, 
allowing users to track their brushing habits and receive personalized recommendations for improving their oral hygiene 
routine. The app provides real-time feedback on brushing technique, ensuring that users are brushing for the recommended two minutes and covering all areas of their mouth.

The toothbrush itself is made from a durable and hygienic material called thermoplastic elastomer. This material is 
non-porous, meaning it doesn't harbor bacteria or mold, making it more hygienic than traditional toothbrush bristles. 
The bristles are also ultra-soft, providing a gentle yet effective clean without causing any damage to the gums or enamel.


In addition to its technological features, the AeroGlide UltraSlim Smart Toothbrush is also eco-friendly. It is designed 
to last for up to six months, reducing the need for frequent replacements. The brush head is also replaceable, further 
reducing waste.

Overall, the AeroGlide UltraSlim Smart Toothbrush by Boie offers a combination of advanced technology, slim design, 
and eco-friendly features. It aims to provide users with a superior brushing experience, helping them maintain optimal 
oral health.


Here is a better prompt to avoid Hallucinations:

prompt = f"""
Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie. 
Before returning an answer determine if the product is real or not.
If there is not such product reply 
``` Great idea for a product but no such product form Boie exists ```
Suggest a similar actual product from Boie.
"""
response = get_completion(prompt)
print(response)

Great idea for a product but no such product from Boie exists. 

A similar actual product from Boie is the Boie USA Toothbrush. It is a high-quality toothbrush made from durable and 
antimicrobial silicone bristles. The bristles are gentle on the gums and teeth, providing a comfortable brushing 
experience. The Boie USA Toothbrush also features a replaceable brush head, reducing waste and making it more 
eco-friendly.

NOTE:  THIS PRODUCT DOES EXIST 



## Iterative Prompt Development

Idea ---> Implement (code / data) Prompt ---> Experimental result ---> Error Analyssi --+
  ^-------------------------------------------------------------------------------------+

guidelines:
        - be clear and specific
        - analyze why result does not give desired output
        - refine idea and prompt
        - repeat


# Starting Prompt
# 
prompt = f"""
Your task is to help a marketing team create a 
description for a retail website of a product based 
on a technical fact sheet.

Write a product description based on the information 
provided in the technical specifications delimited by 
triple backticks.

Technical specifications: ```{fact_sheet_chair}```
"""
response = get_completion(prompt)
print(response)



# Refined Prompt
# 
prompt = f"""
Your task is to help a marketing team create a 
description for a retail website of a product based 
on a technical fact sheet.

Write a product description based on the information 
provided in the technical specifications delimited by 
triple backticks.

The description is intended for furniture retailers, 
so should be technical in nature and focus on the 
materials the product is constructed from.

At the end of the description, include every 7-character 
Product ID in the technical specification.

Use at most 50 words.

Technical specifications: ```{fact_sheet_chair}```
"""
response = get_completion(prompt)
print(response)



# Final Prompt
# 

prompt = f"""
Your task is to help a marketing team create a 
description for a retail website of a product based 
on a technical fact sheet.

Write a product description based on the information 
provided in the technical specifications delimited by 
triple backticks.

The description is intended for furniture retailers, 
so should be technical in nature and focus on the 
materials the product is constructed from.

At the end of the description, include every 7-character 
Product ID in the technical specification.

After the description, include a table that gives the 
product's dimensions. The table should have two columns.
In the first column include the name of the dimension. 
In the second column include the measurements in inches only.

Give the table the title 'Product Dimensions'.

Format everything as HTML that can be used in a website. 
Place the description in a <div> element.

Technical specifications: ```{fact_sheet_chair}```
"""

response = get_completion(prompt)
print(response)




## Summarizing
## 


Summarize with a word / sentence / character limit

prod_review = """
Got this panda plush toy for my daughter's birthday, \
who loves it and takes it everywhere. It's soft and \ 
super cute, and its face has a friendly look. It's \ 
a bit small for what I paid though. I think there \ 
might be other options that are bigger for the \ 
same price. It arrived a day earlier than expected, \ 
so I got to play with it myself before I gave it \ 
to her.
"""


prompt = f"""
Your task is to generate a short summary of a product \
review from an ecommerce site. 

Summarize the review below, delimited by triple 
backticks, in at most 30 words. 

Review: ```{prod_review}```
"""

response = get_completion(prompt)
print(response)



# Summaraize on shipping / delivery

prompt = f"""
Your task is to generate a short summary of a product \
review from an ecommerce site to give feedback to the \
Shipping deparmtment. 

Summarize the review below, delimited by triple 
backticks, in at most 30 words, and focusing on any aspects \
that mention shipping and delivery of the product. 

Review: ```{prod_review}```
"""

response = get_completion(prompt)
print(response)


# Summaraize on pricing

prompt = f"""
Your task is to generate a short summary of a product \
review from an ecommerce site to give feedback to the \
pricing deparmtment, responsible for determining the \
price of the product.  

Summarize the review below, delimited by triple 
backticks, in at most 30 words, and focusing on any aspects \
that are relevant to the price and perceived value. 

Review: ```{prod_review}```
"""

response = get_completion(prompt)
print(response)


# Try "extract" instead of summarize 
prompt = f"""
Your task is to extract relevant information from \ 
a product review from an ecommerce site to give \
feedback to the Shipping department. 

From the review below, delimited by triple quotes \
extract the information relevant to shipping and \ 
delivery. Limit to 30 words. 

Review: ```{prod_review}```
"""

response = get_completion(prompt)
print(response)


# Summaraize multiple reviews  (see L4_summarizing.ipynb)

for i in range(len(reviews)):
    prompt = f"""
    Your task is to generate a short summary of a product \ 
    review from an ecommerce site. 

    Summarize the review below, delimited by triple \
    backticks in at most 20 words. 

    Review: ```{reviews[i]}```
    """

    response = get_completion(prompt)
    print(i, response, "\n")



## Inferring
## 

Reviewing text and extracting labels / names / sentiment 
(see L5_Inferring.ipynb)

# Determine sentiment

prompt = f"""
What is the sentiment of the following product review, 
which is delimited with triple backticks?

Review text: '''{lamp_review}'''
"""
response = get_completion(prompt)
print(response)




prompt = f"""
What is the sentiment of the following product review, 
which is delimited with triple backticks?

Give your answer as a single word, either "positive" \
or "negative".

Review text: '''{lamp_review}'''
"""
response = get_completion(prompt)
print(response)

# Determine emotion

prompt = f"""
Identify a list of emotions that the writer of the \
following review is expressing. Include no more than \
five items in the list. Format your answer as a list of \
lower-case words separated by commas.

Review text: '''{lamp_review}'''
"""
response = get_completion(prompt)
print(response)


# Identify  anger

prompt = f"""
Is the writer of the following review expressing anger?\
The review is delimited with triple backticks. \
Give your answer as either yes or no.

Review text: '''{lamp_review}'''
"""
response = get_completion(prompt)
print(response)


# Infer topics

prompt = f"""
Determine five topics that are being discussed in the \
following text, which is delimited by triple backticks.

Make each item one or two words long. 

Format your response as a list of items separated by commas.

Text sample: '''{story}'''
"""
response = get_completion(prompt)
print(response)



## Transforming 
## 

(see L6_Transforming.ipynb)

LLMs good at transforming between languages, changing grammar, changing formats (HTML, JSON)

# translation

prompt = f"""
Translate the following English text to Spanish: \ 
```Hi, I would like to order a blender```
"""
response = get_completion(prompt)
print(response)


prompt = f"""
Tell me which language this is: 
```Combien coûte le lampadaire?```
"""
response = get_completion(prompt)
print(response)

prompt = f"""
Translate the following  text to French and Spanish
and English pirate: \
```I want to order a basketball```
"""
response = get_completion(prompt)
print(response)


# Universal Translator
user_messages = [
  "La performance du système est plus lente que d'habitude.",  # System performance is slower than normal         
  "Mi monitor tiene píxeles que no se iluminan.",              # My monitor has pixels that are not lighting
  "Il mio mouse non funziona",                                 # My mouse is not working
  "Mój klawisz Ctrl jest zepsuty",                             # My keyboard has a broken control key
  "我的屏幕在闪烁"                                               # My screen is flashing
] 

for issue in user_messages:
    prompt = f"Tell me what language this is: ```{issue}```"
    lang = get_completion(prompt)
    print(f"Original message ({lang}): {issue}")

    prompt = f"""
    Translate the following  text to English \
    and Korean: ```{issue}```
    """
    response = get_completion(prompt)
    print(response, "\n")


# Tone translation

prompt = f"""
Translate the following from slang to a business letter: 
'Dude, This is Joe, check out this spec on this standing lamp.'
"""
response = get_completion(prompt)
print(response)


# Format conversion

data_json = { "resturant employees" :[ 
    {"name":"Shyam", "email":"shyamjaiswal@gmail.com"},
    {"name":"Bob", "email":"bob32@gmail.com"},
    {"name":"Jai", "email":"jai87@gmail.com"}
]}

prompt = f"""
Translate the following python dictionary from JSON to an HTML \
table with column headers and title: {data_json}
"""
response = get_completion(prompt)
print(response)

from IPython.display import display, Markdown, Latex, HTML, JSON
display(HTML(response))


# Spellcheck / Grammar check 
text = [ 
  "The girl with the black and white puppies have a ball.",  # The girl has a ball.
  "Yolanda has her notebook.", # ok
  "Its going to be a long day. Does the car need it’s oil changed?",  # Homonyms
  "Their goes my freedom. There going to bring they’re suitcases.",  # Homonyms
  "Your going to need you’re notebook.",  # Homonyms
  "That medicine effects my ability to sleep. Have you heard of the butterfly affect?", # Homonyms
  "This phrase is to cherck chatGPT for speling abilitty"  # spelling
]
for t in text:
    prompt = f"""Proofread and correct the following text
    and rewrite the corrected version. If you don't find
    and errors, just say "No errors found". Don't use 
    any punctuation around the text:
    ```{t}```"""
    response = get_completion(prompt)
    print(response)

# grammar etc.
text = f"""
Got this for my daughter for her birthday cuz she keeps taking \
mine from my room.  Yes, adults also like pandas too.  She takes \
it everywhere with her, and it's super soft and cute.  One of the \
ears is a bit lower than the other, and I don't think that was \
designed to be asymmetrical. It's a bit small for what I paid for it \
though. I think there might be other options that are bigger for \
the same price.  It arrived a day earlier than expected, so I got \
to play with it myself before I gave it to my daughter.
"""
prompt = f"proofread and correct this review: ```{text}```"
response = get_completion(prompt)
print(response)


# redline it
from redlines import Redlines

diff = Redlines(text,response)
display(Markdown(diff.output_markdown))




## Expanding 
## 
(see L7_Expanding.ipynb)

Expand a reply from a shorter prompt, i.e. writing an email

prompt = f"""
You are a customer service AI assistant.
Your task is to send an email reply to a valued customer.
Given the customer email delimited by ```, \
Generate a reply to thank the customer for their review.
If the sentiment is positive or neutral, thank them for \
their review.
If the sentiment is negative, apologize and suggest that \
they can reach out to customer service. 
Make sure to use specific details from the review.
Write in a concise and professional tone.
Sign the email as `AI customer agent`.
Customer review: ```{review}```
Review sentiment: {sentiment}
"""
response = get_completion(prompt)
no_temp=response
print(response)

response = get_completion(prompt, temperature=0.7)
temp=response
print(response)

# now show diffs between temp=0.0 and temp=0.7 
# 
from redlines import Redlines
from IPython.display import display, Markdown, Latex, HTML, JSON

diff = Redlines(no_temp, temp)
display(Markdown(diff.output_markdown))


NOTE: Temperature zero is a more "predictable" response.  The higher the temperature to 
      more likely it will return a word or phrase that would normally not be chosen.
        temp = 0.0  ==>> tasks that require reliability, predictability
        temp = 0.7  ==>> wider variety of outputs




## Chatbot 
## 
(see L8_Chatbot.ipynb)

Expanding on the original get_completion function.  get_completion_from_messages gets a number of 
messages passed to it with.  
    i.e. 
        system:    sets behavior of the assistant (whister in the assistant's ear)
        user:      input from the users
        assistant: The LLM / ChatGPT
         
def get_completion_from_messages(messages, model="gpt-3.5-turbo", temperature=0):
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, # this is the degree of randomness of the model's output
    )
#     print(str(response.choices[0].message))
    return response.choices[0].message["content"]

messages =  [  
{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    
{'role':'user', 'content':'tell me a joke'},   
{'role':'assistant', 'content':'Why did the chicken cross the road'},   
{'role':'user', 'content':'I don\'t know'}  ]


NOTE: Each conversation is a stand alone interaction.  Must provide all relevant messages from
      from earlier in the current conversation.  This is "context".



## Conclusion 
## 


#########################################################################################################
## LangChain for LLM Application Development
#########################################################################################################

https://learn.deeplearning.ai/langchain/lesson/1/introduction


## Introduction 
## 
    - Framework for LLM apps
    - Focused on composition / modularity 
            - Modular components
            - Common wyas to combine components

    - Models    
            - 20+ integrations

    - Prompts    
            - Prompt Templates
            - Output parsers
            - Example selectors

    - Indexes
            - Document Loaders
            - Text Splitters
            - Vector stores

    - Chains
            - Prompt + LLM + Output parsing

    - Agents
            - Agent types



## Models, Prompts and Parsers 
## 

(see LC1_Model_prompt_parser.ipynb)


## import langchain
from langchain.chat_models import ChatOpenAI


# Create chat endpoing
# To control the randomness and creativity of the generated
# text by an LLM, use temperature = 0.0
chat = ChatOpenAI(temperature=0.0, model="gpt-3.5-turbo")

# import langchain Chat Prompt Template
from langchain.prompts import ChatPromptTemplate
## bring in chat prompt template
template_string = """Translate the text \
that is delimited by triple backticks \
into a style that is {style}. \
text: ```{text}```
"""

# create the template
prompt_template = ChatPromptTemplate.from_template(template_string)

# create values for place holders style and email
customer_style = """American English \
in a calm and respectful tone
"""

customer_email = """
Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls \
with smoothie! And to make matters worse,  the warranty don't cover the cost of \
cleaning up me kitchen. I need yer help right now, matey! 
"""

# format the prompt template with the values
customer_messages = prompt_template.format_messages(
                    style=customer_style,
                    text=customer_email)

# send formatted prompt to chat
customer_response = chat(customer_messages)


## import Response Schema and Structured Output Parser
from langchain.output_parsers import ResponseSchema
from langchain.output_parsers import StructuredOutputParser
## set up schemas for the the parts of the response

gift_schema = ResponseSchema(name="gift",
                             description="Was the item purchased\
                             as a gift for someone else? \
                             Answer True if yes,\
                             False if not or unknown.")
delivery_days_schema = ResponseSchema(name="delivery_days",
                                      description="How many days\
                                      did it take for the product\
                                      to arrive? If this \
                                      information is not found,\
                                      output -1.")
price_value_schema = ResponseSchema(name="price_value",
                                    description="Extract any\
                                    sentences about the value or \
                                    price, and output them as a \
                                    comma separated Python list.")

response_schemas = [gift_schema, 
                    delivery_days_schema,
                    price_value_schema]


## create the structured output parser

## get structured output parser
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

## get instructions from output parser
format_instructions = output_parser.get_format_instructions()

## set review template
review_template_2 = """\
For the following text, extract the following information:

gift: Was the item purchased as a gift for someone else? \
Answer True if yes, False if not or unknown.

delivery_days: How many days did it take for the product\
to arrive? If this information is not found, output -1.

price_value: Extract any sentences about the value or price,\
and output them as a comma separated Python list.

text: {text}

{format_instructions}
"""

## set the prompt from the template
prompt = ChatPromptTemplate.from_template(template=review_template_2)

## prompt with the formatted templates
messages = prompt.format_messages(text=customer_review, 
                                format_instructions=format_instructions)

## call chat with the formatted prompt
response = chat(messages)

## now parse the output: response.content
output_dict = output_parser.parse(response.content)


output_dict

{'gift': False,
 'delivery_days': '2',
 'price_value': "It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features."
}
output_dict.get('delivery_days')



## Memory (17 min)
## 
(see LC2_Memory.ipynb)

https://learn.deeplearning.ai/langchain/lesson/3/memory

##from langchain.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

llm_model="gpt-3.5-turbo"
llm = ChatOpenAI(temperature=0.0, model="gpt-3.5-turbo")
memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=True
)

- LLMs are stateless 
    - each transaction is independent
    - chatbots appear to have memory by providing the full conversation as 'context'
        - i.e. memory of current chat / interaction is kept in a buffer

- Langcahin offers various Conversation buffers:
# ConversationBufferMemory:       keeps current conversation in memory
        - can update current memory context as such:
                memory.save_context({"input": "Not much, just hanging"}, {"output": "Cool"})
                memory.load_memory_variables({})
                > {'history': 'Human: Hi, my name is Doktor Z\nAI: Hello Doktor Z! How can I assist you today?\n
                    Human: What is 1+1?\nAI: 1+1 is equal to 2.\nHuman: What is my name?\nAI: Your name is Doktor Z.\n
                    Human: Not much, just hanging\nAI: Cool'}


    - The bigger the memory buffer the more tokens must be processed to proces the history of the chat

 # ConversationBufferWindowMemory: keeps a "windows" of current conversation i.e. last N items, etc.
            ## This only remembers the last k=1 items in the conversation
            ## 
            llm = ChatOpenAI(temperature=0.0, model=llm_model)
            memory = ConversationBufferWindowMemory(k=1)
            conversation = ConversationChain(
                llm=llm, 
                memory = memory,
                verbose=False
            )


# ConversationTokenBufferMemory:  keeps certain number of tokens of current conversation 
        NOTE: install via pip install toktoken
        - Like WindowMemory but keeps a token limit in memory (history)

            ## This only remembers the 50 tokens worth of conversation
            ## 
            from langchain.memory import ConversationTokenBufferMemory
            from langchain.llms import OpenAI
            llm = ChatOpenAI(temperature=0.0, model=llm_model)
            memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)



# ConversationSummaryMemory:      keeps a running summary of current conversation 
        - Has the LLM to provide a summary of the conversaiont up to N tokens

            # create a long string
            schedule = "There is a meeting at 8am with your product team. \
            You will need your powerpoint presentation prepared. \
            9am-12pm have time to work on your LangChain \
            project which will go quickly because Langchain is such a powerful tool. \
            At Noon, lunch at the italian resturant with a customer who is driving \
            from over an hour away to meet you to understand the latest in AI. \
            Be sure to bring your laptop to show the latest LLM demo."

            memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)
            memory.save_context({"input": "Hello"}, {"output": "What's up"})
            memory.save_context({"input": "Not much, just hanging"},
                                {"output": "Cool"})
            memory.save_context({"input": "What is on the schedule today?"}, 
                                {"output": f"{schedule}"})
            memory.load_memory_variables({})

            ## Summary generated
            > {'history': 'System: The human and AI exchange greetings. The human asks about the schedule for the day. 
                       The AI provides a detailed schedule, including a meeting with the product team, work on the 
                       LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes 
                       the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.'}



- Additional Langcahin Memory Types :
    - Vector data memory:  Stores text to a vector db and retrieves most relevant blocks of text
    - Entity memories :    Using an LLM, remembers details about specific entities.

NOTE: can use multiple memories at same time and can store in conventional database



## Chains (13 min)
## 
 - chain together LLM and other actions
        # set model, prompts and chain 
        from langchain.chat_models import ChatOpenAI
        from langchain.prompts import ChatPromptTemplate
        from langchain.chains import LLMChain

        # set llm
        llm = ChatOpenAI(temperature=0.9, model="gpt-3.5-turbo")

        # set prompt
        prompt = ChatPromptTemplate.from_template(
                "What is the best name to describe \
                a company that makes {product}?"
        )
        # set chain
        chain = LLMChain(llm=llm, prompt=prompt)

        # now set prompat and run against llm (via chain!)
        product = "Queen Size Sheet Set"
        chain.run(product)


# Sequential Chains
    - Sequential chain is a type of chain.  Combine multiple chians where the 
      output of the one chain is the input of the next chain. (think unix pipelines)

    Two Types of Sequential Chains:
    1. SimpleSequentialChain: Single input / output 
    1. SequentialChain:       Multiple inputs / outputs 
[2:53]

# SimpleSequentialChain

  [Input] --Chain_1--> [Output_Chain_1] = [Input_Chain_2] --Chain_2--> [Output_Chain_2]


    ex. 
        # set llm
        llm = ChatOpenAI(temperature=0.9, model="gpt-3.5-turbo")

        # set prompt template 1
        first_prompt = ChatPromptTemplate.from_template(
                "What is the best name to describe \
                a company that makes {product}?"
        )

        # set chain 1
        chain_one = LLMChain(llm=llm, prompt=first_prompt)

        # set prompt template 2
        second_prompt = ChatPromptTemplate.from_template(
            "Write a 20 words description for the following \
            company:{company_name}"
        )

        # set chain 2
        chain_two = LLMChain(llm=llm, prompt=second_prompt)

        # link the chains
        # run the linked chains: output of chain 1 is the input of chain 2.
        overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)

        # run the chain                       
        overall_simple_chain.run(product)

# SequentialChain
  
           +--Chain_1--> [Out_Chain_1] = [In_Chain_2] --Chain_2--> [Out_Chain_2] = [In_Chain_4]
          /                                                                               + 
  [Input]                                                                                 +----Chain_4--> [Out_Chain_4]
          \                                                                               +
           +-------------------------Chain_3---------------------> [Out_Chain_3] = [In_Chain_4] 



    - This creates a chain with several steps withthe output of one feeding input of the next chain
    
            from langchain.chains import SequentialChain
            llm = ChatOpenAI(temperature=0.9, model="gpt-3.5-turbo")

            # prompt template 1: translate to english
            first_prompt = ChatPromptTemplate.from_template("Translate the following review to english:"
                            "\n\n{Review}" )
            
            # chain 1: input= Review and output= English_Review
            chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key="English_Review")
                    

            # 2nd prompt template 1: summarize
            second_prompt = ChatPromptTemplate.from_template(
                    "Can you summarize the following review in 1 sentence:"
                    "\n\n{English_Review}")

            # chain 2: input= English_Review and output= summary
            chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key="summary")
                    
            # 3rd prompt template 3: what language english
            third_prompt = ChatPromptTemplate.from_template( "What language is the following review:\n\n{Review}")

            # chain 3: input= Review and output= language
            chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key="language")
                      
            # prompt template 4: tranlate the summary to the original language 
            fourth_prompt = ChatPromptTemplate.from_template(
                    "Write a follow up response to the following "
                    "summary in the specified language:"
                    "\n\nSummary: {summary}\n\nLanguage: {language}"
            )

            # chain 4: input= summary, language and output= followup_message
            chain_four = LLMChain(llm=llm, prompt=fourth_prompt,output_key="followup_message" )

            # Overall Chain == chain 1 + chain 2 + chain 3 + chain 4
            # overall_chain: input= Review 
            # and output= English_Review,summary, followup_message
            overall_chain = SequentialChain( chains=[chain_one, chain_two, chain_three, chain_four],
                        input_variables=["Review"],output_variables=["English_Review", "summary","followup_message"],
                        verbose=True)

            ## get a review and input into the overall chain             
            review = df.Review[5]
            overall_chain(review)



# Router Chain
 see example in LC3_Chains.ipynb

    - Route the input of a chain depending on what that input is
  
       +--------Router Chain-------+      If input     +- [ Subject ] --Destination_Chain --> [ Ouput ]
       |     +----subjects---+     |   is related to  /                                                                               
       |     |  1. Math      |     |                 /                                                                               
  [Input]    |  2. History   | ----|----------------+                                                                
       |     |  3. ....      |     |                 \                                                                               
       |     +----------------     |         else     \                                                                               
       +---------------------------+                   +--- [ None ] -------Default_Chain ---> [ Ouput ]


## Question and Answer (15 min)
## 
 see example in LC4_QnA.ipynb

 - Q & A over Documents 
    - combining LLM with data not originally chained on

LLMs on Documents:
    - LLMs can only inspect a few thousand words at a time

    Embeddings: 
        - create numeric representations for pieces of text
            - captures semantic meaning of piece of text
        - text with similar content will have similar vectors
            - allows you to "compare" pieces of text in the vector space 

    Vector Database: 
        - populated with chunks of text from incoming documents

              Create Vector Db

    +--[Document]--+                                   +------------------------------------------+ 
    | ...........  |  ->  [chunk] -> [embed]   -->>    | [-0.0035, -0.108 .. 0.063]     [.....]   |
    | ...........  |  ->  [chunk] -> [embed]   -->>    | [-0.0035, -0.818 .. 0.966]     [.....]   |
    | ...........  |  ->  [chunk] -> [embed]   -->>    | [ .......................]     [.....]   |
    | ...........  |  ->  [chunk] -> [embed]   -->>    | [ .......................]     [.....]   |
    | ...........  |  ->  [chunk] -> [embed]   -->>    | [ .......................]     [.....]   |
    | ...........  |  ->  [chunk] -> [embed]   -->>    | [ .......................]     [.....]   |
    | ...........  |  ->  [chunk] -> [embed]   -->>    | [-0.4724, -0.428 .. 0.097]     [.....]   |
    |              |                                   +------------------------------------------+
    +--------------+                                         embedding vectors      original chunks


              Index 
               
                                                       +------------------------------------------+ 
                  +---->  [chunk] -> [embed]   -->>    | [-0.0035, -0.108 .. 0.063]     [.....]   |
                /    +->  [chunk] -> [embed]   -->>    | [-0.0035, -0.818 .. 0.966]     [.....]   |  --> [......]
               /    /                                  | [ .......................]     [.....]   |
              /    /                                   | [ .......................]     [.....]   |
        +----------+                                   | [ .......................]     [.....]   |
        |  query   |                                   | [ .......................]     [.....]   |
        |          |--->  [chunk] -> [embed]   -->>    | [-0.4724, -0.428 .. 0.097]     [.....]   |  --> [......]
        +----------|                                   +------------------------------------------+
                                                                                                 Pick the N most similar 
                           Compare All Entries   

    NOTE: Pass the returned entries from Vector DB query and pass to LLM


How to setup vector db:

import os
from dotenv import load_dotenv, find_dotenv
## get env
## 
_ = load_dotenv(find_dotenv("/home/map/archive/ai/ai_courses/code/.env")) # read local .env file

import warnings
warnings.filterwarnings('ignore')

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import CSVLoader
from langchain.vectorstores import DocArrayInMemorySearch
from IPython.display import display, Markdown
##from langchain.llms import OpenAI
from langchain_openai import OpenAI

## Load document file
## 
file = '/home/map/archive/ai/ai_courses/data/OutdoorClothingCatalog_750.csv'
from langchain.document_loaders import CSVLoader
loader = CSVLoader(file_path=file)
docs = loader.load()

## Get Embeddings of document file
## 
from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()

## Create vector store from docs and embeddings object
## 
db = DocArrayInMemorySearch.from_documents(
    docs, 
    embeddings
)

## Now query against Vector DB
## 
query = "Please suggest a shirt with sunblocking"
docs = db.similarity_search(query)
len(docs)

## Create "retriever" from vector store
## generic interface that takes in query and returns documents
## 
retriever = db.as_retriever()

llm = ChatOpenAI(temperature = 0.0, model="gpt-3.5-turbo")

## Create Retrieval QnA Chain
qa_stuff = RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type="stuff", 
    retriever=retriever, 
    verbose=True
)

query =  "Please list all your shirts with sun protection in a table \
in markdown and summarize each one."

response = qa_stuff.run(query)
display(Markdown(response))


## can also do it as 
## response = index.query(query, llm=llm)
##  
## or
##  
## index = VectorstoreIndexCreator(
##     vectorstore_cls=DocArrayInMemorySearch,
##     embedding=embeddings,
## ).from_loaders([loader])


Stuff method:  stuff all data into the prompt as context to pass to language model.
    pros: makes a single call to LLM. LLM has access to all the data at once
    cons: LLMs have a context length.  For large or many docs this will not work 
            will result in a prompt larger than context length


3 Additional methods:
    - Map_reduce
        - chunks to LLM and then assemble at final LLM
    - Refine
        - iteratively over chunks
        NOTE: not as fast, waits for results of previous calls
    - Map_rerank
        - chunks to LLM and aks for score and return highest score 

NOTE: use Map_reduce chain to summarize a document


## Evaluation (15 min)
## 

MAP LAST HERE 

## Agents (14 min)
## 


## Conclusion (2 min)
## 











.